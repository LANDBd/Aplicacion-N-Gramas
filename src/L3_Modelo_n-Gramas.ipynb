{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Entrenamiento**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "Antes de entrenar los modelos de n-gramas, realizaremos un procedimiento para indicar que tan seguido las distintas oraciones empiezan o terminan con cierto token. \n",
    "\n",
    "La idea general es agregar el token \\<s> al inicio de cada oración, y \\</s> al final de estas. Para esto, se utiliza la función padded_everygram_pipeline, la cual recibe el orden de n-gramas mas alto, y el texto donde se va a iterar la función.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def saveCorpus(corpus: list[list], path: str) -> None:\n",
    "    \"\"\" Guarda un corpus generado al correr esta libreta en un archivo txt definido en path \"\"\"\n",
    "    with open(path, mode='a', encoding='utf-8') as f:\n",
    "        for sentence in corpus:\n",
    "            f.write(\" \".join(word for word in sentence) + '\\n')\n",
    "    print(\"done.\")\n",
    "\n",
    "def getCorpus(path: str) -> list[list]:\n",
    "    \"\"\" Recupera un corpus guardado en un archivo de txt \"\"\"\n",
    "    corpus = []\n",
    "    with open(path, mode='r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            corpus.append(line.split())\n",
    "    print(\"done.\")\n",
    "    return corpus\n",
    "\n",
    "import random\n",
    "random.seed(2023)\n",
    "def splitCorpus(corpus: list[list], splitFactor=0.2) -> tuple:\n",
    "    \"\"\"\" Separa los datos en un conjunto de entenamiento y uno de prueba, \n",
    "         de tal forma que si splitFactor = 0.2 entonces un 20% estará en \n",
    "         el conjunto de prueba y el restante el de entrenamiento. \"\"\"\n",
    "    splitFactor = max(0.2, splitFactor)\n",
    "    N = len(corpus)\n",
    "\n",
    "    randomCorpus = corpus.copy()\n",
    "    random.shuffle(randomCorpus)\n",
    "    trainCorpus = randomCorpus[int((N+1)*splitFactor):]\n",
    "    testCorpus = randomCorpus[:int((N+1)*splitFactor)]\n",
    "\n",
    "    return trainCorpus, testCorpus\n",
    "\n",
    "datasetPath = os.path.join(os.getcwd(), \"data\")\n",
    "data = getCorpus(datasetPath + \"/cleanCorpus.txt\")\n",
    "X_train, X_test = splitCorpus(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192716 oraciones en el conjunto de entrenamiento.\n",
      "48179 en el de pruebas.\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "print(f\"{len(X_train)} oraciones en el conjunto de entrenamiento.\\n{len(X_test)} en el de pruebas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "train1,vocab1=padded_everygram_pipeline(1,X_train)\n",
    "train2,vocab2=padded_everygram_pipeline(2,X_train)\n",
    "train3,vocab3=padded_everygram_pipeline(3,X_train)\n",
    "train4,vocab4=padded_everygram_pipeline(3,X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Despues del procedimiento previo, procedemos a entrenar los modelos. Entrenaremos el Estimador de Máxima Verosimilitud (Maximum Likelihood Estimator o MLE), especificando el orden más alto de n-gramas a utilizar. Además, en el penúltimo modelo se aplicará suavizado de Laplace, y en el último el suavizado de Kneser-Ney.\n",
    "\n",
    "La función MLE crea un vocabulario vacío, el cual se llena al ajustar el modelo con la función fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.lm import Laplace\n",
    "from nltk.lm import AbsoluteDiscountingInterpolated\n",
    "\n",
    "Lm1=MLE(1)\n",
    "Lm2=MLE(2)\n",
    "Lm3=Laplace(3)\n",
    "Lm4=AbsoluteDiscountingInterpolated(3)\n",
    "\n",
    "Lm1.fit(train1,vocab1)\n",
    "Lm2.fit(train2,vocab2)\n",
    "Lm3.fit(train3,vocab3)\n",
    "Lm4.fit(train4,vocab4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluando y utilizando modelos entrenados\n",
    "\n",
    "Para evaluar a nuestros modelos, no utilizamos la probabilidad como una métrica, si no la perplejidad,\n",
    "la cual es una cantidad que nos dice que tan bien una distribución de probabilidad o un modelo predice una muestra. Cuando un modelo de n-gramas predice bien a la muestra, la perplejidad estará mas cerca a 1.\n",
    "\n",
    "En la evaluación del modelo se utiliza el conjunto de prueba test, utilizando la función lm.perplexity(test).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La perplejidad del modelo de unigramas es: inf\n",
      "La perplejidad del modelo de bigramas es: inf\n",
      "La perplejidad del modelo con suavizado de Laplace es: 39605.85358237703\n",
      "La perplejidad del modelo con suavizado de Laplace es: inf\n"
     ]
    }
   ],
   "source": [
    "print(\"La perplejidad del modelo de unigramas es: \" + str(Lm1.perplexity(X_test)))\n",
    "print(\"La perplejidad del modelo de bigramas es: \" + str(Lm2.perplexity(X_test)))\n",
    "print(\"La perplejidad del modelo con suavizado de Laplace es: \" + str(Lm3.perplexity(X_test)))\n",
    "print(\"La perplejidad del modelo con suavizado de Laplace es: \" + str(Lm4.perplexity(X_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generación de frases.\n",
    "\n",
    "Se utiliza `lm.generate(Numero de palabras, random seed)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase con modelo de unigramas:\n",
      "ciento las ya lo y un va de lo pero la propósito estuvimos Vida como \n"
     ]
    }
   ],
   "source": [
    "print(\"Frase con modelo de unigramas:\")\n",
    "\n",
    "text=\"\"\n",
    "frase=Lm1.generate(15)\n",
    "for word in frase:\n",
    "    text=text+word+\" \"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase con modelo de bigramas:\n",
      "relacionado con crecimiento rescate </s> a conocer y vamos a estas ciudades del pasado ejerció \n"
     ]
    }
   ],
   "source": [
    "print(\"Frase con modelo de bigramas:\")\n",
    "\n",
    "text=\"\"\n",
    "frase=Lm2.generate(15)\n",
    "for word in frase:\n",
    "    text=text+word+\" \"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase con modelo de suavizado de Laplace:\n",
      "debería de existir algún oficio en donde perdían y en el informe sobre la Marina \n"
     ]
    }
   ],
   "source": [
    "print(\"Frase con modelo de suavizado de Laplace:\")\n",
    "\n",
    "text=\"\"\n",
    "frase=Lm3.generate(15)\n",
    "for word in frase:\n",
    "    text=text+word+\" \"\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase con modelo de suavizado de librería:\n",
      "tercera dosis empezando con los maestros de Campeche del mar y se para el mundo \n"
     ]
    }
   ],
   "source": [
    "print(\"Frase con modelo de suavizado de librería:\")\n",
    "\n",
    "text=\"\"\n",
    "frase=Lm4.generate(15)\n",
    "for word in Lm4.generate(15):\n",
    "    text=text+word+\" \"\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones finales\n",
    "\n",
    "En este trabajo abordamos la tarea de generar texto mediante un modelo de lenguaje basado en *n-gramas*, con el objetivo de producir oraciones en base a las mañaneras del presidente de México Lic. Andrés Manuel López Obrador.\n",
    "\n",
    "La metodología fue:\n",
    "1. Obtención de datos.\n",
    "2. Preparación y análisis exploratorio de datos.\n",
    "3. Elaboración del modelo.\n",
    "\n",
    "### Obtención de datos\n",
    "\n",
    "Para la obtención de los datos crudos utilizamos como fuente el repositorio de [@nostradata ](https://www.nostrodata.com) en [`GitHub`](https://github.com/NOSTRODATA/conferencias_matutinas_amlo) en la cual se encuentran transcripciones de múltiples mañaneras separadas por fecha y participante. Se utilizó una librería que nos permitia descargar contenido del repositorio mediante Python.\n",
    "\n",
    "### Preparación y análisis exploratorio de datos\n",
    "\n",
    "Una vez que se tuvo todo el corpus en un archivo de .txt se realizo una segmentación tanto por palabras como oraciones con el objetivo de realizar un análisis que nos permitiera tomar desiciones con respecto al procesamiento de los datos. Entre ellos se realizaron estadísticas simples sobre las palabras y oraciones segmentadas, como la distribución de tamaños y frecuencias, y gráficas para darse una idea del corpus.\n",
    "\n",
    "Se tomó la desición de conservar únicamente carácteres alfanuméricos y de convertir en tokens `<UNK>` aquellas palabras poco frecuentes que aparecieran menos de 4 veces las desviación estandadar con respecto a la media.\n",
    "\n",
    "Se realizon un separación de conjunto de entrenamiento del 80% de las oraciones y un 20% para el conjunto de pruebas.\n",
    "\n",
    "### Elaboración del modelo\n",
    "\n",
    "Estuvo bien padriuks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3e10ef16274dd72e574b8fa73b58450b957d8421a2901baded3cca26fcf5dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
