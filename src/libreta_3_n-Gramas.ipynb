{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Entrenamiento**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "Antes de entrenar los modelos de n-gramas, realizaremos un procedimiento para indicar que tan seguido las distintas oraciones empiezan o terminan con cierto token. \n",
    "\n",
    "La idea general es agregar el token \\<s> al inicio de cada oración, y \\</s> al final de estas. Para esto, se utiliza la función padded_everygram_pipeline, la cual recibe el orden de n-gramas mas alto, y el texto donde se va a iterar la función.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "text=[['La', 'mafia', 'del', 'poder', 'me', 'ataca', 'sin', 'cesar']]\n",
    "test=[]\n",
    "\n",
    "train1,vocab1=padded_everygram_pipeline(1,text)\n",
    "train2,vocab2=padded_everygram_pipeline(2,text)\n",
    "train3,vocab3=padded_everygram_pipeline(2,text)\n",
    "train4,vocab4=padded_everygram_pipeline(2,text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Despues del procedimiento previo, procedemos a entrenar los modelos. Entrenaremos el Estimador de Máxima Verosimilitud (Maximum Likelihood Estimator o MLE), especificando el orden más alto de n-gramas a utilizar. Además, en el penúltimo modelo se aplicará suavizado de Laplace, y en el último el suavizado de Kneser-Ney.\n",
    "\n",
    "La función MLE crea un vocabulario vacío, el cual se llena al ajustar el modelo con la función fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.lm import Laplace\n",
    "from nltk.lm import KneserNeyInterpolated\n",
    "\n",
    "Lm1=MLE(1)\n",
    "Lm2=MLE(2)\n",
    "Lm3=Laplace(2)\n",
    "Lm4=KneserNeyInterpolated(2)\n",
    "\n",
    "Lm1.fit(train1,vocab1)\n",
    "Lm2.fit(train2,vocab2)\n",
    "Lm3.fit(train3,vocab3)\n",
    "Lm4.fit(train4,vocab4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluando y utilizando modelos entrenados\n",
    "\n",
    "Para evaluar a nuestros modelos, no utilizamos la probabilidad como una métrica, si no la perplejidad,\n",
    "la cual es una cantidad que nos dice que tan bien una distribución de probabilidad o un modelo predice una muestra. Cuando un modelo de n-gramas predice bien a la muestra, la perplejidad estará mas cerca a 1.\n",
    "\n",
    "En la evaluación del modelo se utiliza el conjunto de prueba test, utilizando la función lm.perplexity(test).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La perplejidad del modelo de unigramas es: \" + str(Lm1.perplexity(test)))\n",
    "print(\"La perplejidad del modelo de bigramas es: \" + str(Lm2.perplexity(test)))\n",
    "print(\"La perplejidad del modelo con suavizado de Laplace es: \" + str(Lm3.perplexity(test)))\n",
    "print(\"La perplejidad del modelo con suavizado de Kneser-Ney es: \" + str(Lm4.perplexity(test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generación de frases.\n",
    "\n",
    "Se utiliza lm.generate(Numero de palabras, random seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frases con modelo de unigramas:\n",
      "poder me cesar cesar poder poder sin \n",
      "sin mafia poder poder La sin del \n",
      "del cesar poder cesar poder del me \n",
      "\n",
      "\n",
      "Frases con modelo de bigramas:\n",
      "cesar </s> del poder me ataca sin \n",
      "<s> La mafia del poder me ataca \n",
      "mafia del poder me ataca sin cesar \n",
      "\n",
      "\n",
      "Frases con modelo de suavizado de Laplace:\n",
      "cesar </s> del poder me ataca sin \n",
      "mafia del poder me ataca sin cesar \n",
      "cesar </s> del poder me ataca sin \n",
      "\n",
      "\n",
      "Frases con modelo de suavizado de Kneser-Ney:\n",
      "La mafia del poder me ataca sin \n",
      "sin cesar </s> me ataca sin cesar \n",
      "cesar </s> cesar </s> poder me ataca \n"
     ]
    }
   ],
   "source": [
    "print(\"Frases con modelo de unigramas:\")\n",
    "for i in range(3):\n",
    "    text=\"\"\n",
    "    for word in Lm1.generate(7):\n",
    "        text=text+word+\" \"\n",
    "    print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Frases con modelo de bigramas:\")\n",
    "for i in range(3):\n",
    "    text=\"\"\n",
    "    for word in Lm2.generate(7):\n",
    "        text=text+word+\" \"\n",
    "    print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Frases con modelo de suavizado de Laplace:\")\n",
    "for i in range(3):\n",
    "    text=\"\"\n",
    "    for word in Lm3.generate(7):\n",
    "        text=text+word+\" \"\n",
    "    print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Frases con modelo de suavizado de Kneser-Ney:\")\n",
    "for i in range(3):\n",
    "    text=\"\"\n",
    "    for word in Lm4.generate(7):\n",
    "        text=text+word+\" \"\n",
    "    print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones finales"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3e10ef16274dd72e574b8fa73b58450b957d8421a2901baded3cca26fcf5dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
